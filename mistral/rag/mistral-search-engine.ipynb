{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m2A7FD52TNq"
      },
      "source": [
        "## Mistral AI search engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "X8wgZcKS2TNs"
      },
      "outputs": [],
      "source": [
        "%pip install uv -q\n",
        "!uv pip install --system -q aiohttp==3.9.5 beautifulsoup4==4.12.3 faiss_cpu==1.8.0 mistralai==0.4.0 nest_asyncio==1.6.0 numpy==1.26.4 pandas==2.2.2 python-dotenv==1.0.1 requests==2.32.3\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6sZo0LJ2TNt"
      },
      "source": [
        "![image info](https://github.com/mistralai/cookbook/blob/main/images/mistral-search-graph.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hjE9p9bj2TNt"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()  # load environment variables from .env file\n",
        "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
        "\n",
        "from google.colab import userdata\n",
        "if not MISTRAL_API_KEY:\n",
        "  MISTRAL_API_KEY = userdata.get('MISTRAL_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rN20mtlq2TNv"
      },
      "source": [
        "## Scraper Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MSKn8MEN2TNv",
        "outputId": "259dcdd7-1037-437f-9ade-7868371417da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 1\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 2\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 3\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 4\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 5\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 6\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 7\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 8\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 9\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 10\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 11\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 12\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 13\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 14\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 15\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 16\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 17\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 18\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 19\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 20\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 21\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 22\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 23\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 24\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 25\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 26\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 27\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 28\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 29\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 30\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 31\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 32\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 33\n",
            "len results is :  0  and total to fetch is  10\n",
            "Fetching page: 34\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "CancelledError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d923d5e24d39>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfaiss_index_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m \u001b[0;32mawait\u001b[0m \u001b[0mfetch_and_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"What is the latest news about AI and comfyui in the last 10 days?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d923d5e24d39>\u001b[0m in \u001b[0;36mfetch_and_process_data\u001b[0;34m(search_query)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mpage_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'len results is : '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' and total to fetch is '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_results_to_fetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mawait\u001b[0m \u001b[0mfetch_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'links'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d923d5e24d39>\u001b[0m in \u001b[0;36mfetch_page\u001b[0;34m(session, params, page_num, results)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Fetching page: {page_num}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpage_num\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"https://www.google.com/search\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-d923d5e24d39>\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(session, url, params)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32masync\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/client.py\u001b[0m in \u001b[0;36m__aenter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m__aenter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_RetType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, method, str_or_url, params, data, json, cookies, headers, skip_auto_headers, auth, allow_redirects, max_redirects, compress, chunked, expect100, raise_for_status, read_until_eof, proxy, proxy_auth, timeout, verify_ssl, fingerprint, ssl_context, ssl, server_hostname, proxy_headers, trace_request_ctx, read_bufsize, auto_decompress, max_line_size, max_field_size)\u001b[0m\n\u001b[1;32m    606\u001b[0m                             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m                                 \u001b[0;32mawait\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m                             \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m                                 \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/client_reqrep.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, connection)\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m                     \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protocol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m                     \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpayload\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHttpProcessingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m                     raise ClientResponseError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/aiohttp/streams.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m                 \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCancelledError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_waiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/asyncio/futures.py\u001b[0m in \u001b[0;36m__await__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_asyncio_future_blocking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m  \u001b[0;31m# This tells Task to wait for completion.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"await wasn't used with future\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCancelledError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import requests\n",
        "import re\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import numpy as np\n",
        "from mistralai.client import MistralClient\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "\n",
        "# Apply the nest_asyncio patch\n",
        "nest_asyncio.apply()\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36\"\n",
        "}\n",
        "\n",
        "total_results_to_fetch = 100  # total number of results to fetch\n",
        "chunk_size = 1000  # size of each text chunk\n",
        "\n",
        "dataframe_out_path = 'temp.csv'\n",
        "faiss_index_path = 'faiss_index.index'\n",
        "\n",
        "mistral_api_key = MISTRAL_API_KEY  # replace with your actual API key\n",
        "client = MistralClient(api_key=mistral_api_key)\n",
        "\n",
        "async def fetch(session, url, params=None):\n",
        "    async with session.get(url, params=params, headers=headers, timeout=30) as response:\n",
        "        return await response.text()\n",
        "\n",
        "async def fetch_page(session, params, page_num, results):\n",
        "    print(f\"Fetching page: {page_num}\")\n",
        "    params[\"start\"] = (page_num - 1) * params[\"num\"]\n",
        "    html = await fetch(session, \"https://www.google.com/search\", params)\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    for result in soup.select(\".tF2Cxc\"):\n",
        "        if len(results) >= total_results_to_fetch:\n",
        "            break\n",
        "        title = result.select_one(\".DKV0Md\").text\n",
        "        links = result.select_one(\".yuRUbf a\")[\"href\"]\n",
        "\n",
        "        results.append({\n",
        "            \"title\": title,\n",
        "            \"links\": links\n",
        "        })\n",
        "\n",
        "async def fetch_content(session, url):\n",
        "    async with session.get(url, headers=headers, timeout=30) as response:\n",
        "        return await response.text()\n",
        "\n",
        "async def fetch_all_content(urls):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [fetch_content(session, url) for url in urls]\n",
        "        return await asyncio.gather(*tasks)\n",
        "\n",
        "def get_all_text_from_url(url):\n",
        "    response = requests.get(url, headers=headers, timeout=30)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    for script in soup([\"script\", \"style\"]):\n",
        "        script.extract()\n",
        "    text = soup.get_text()\n",
        "    lines = (line.strip() for line in text.splitlines())\n",
        "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "    return text\n",
        "\n",
        "def split_text_into_chunks(text, chunk_size):\n",
        "    sentences = re.split(r'(?<=[.!?]) +', text)\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sum(len(s) for s in current_chunk) + len(sentence) + 1 > chunk_size:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [sentence]\n",
        "        else:\n",
        "            current_chunk.append(sentence)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "async def process_text_content(texts, chunk_size):\n",
        "    loop = asyncio.get_event_loop()\n",
        "    tasks = [loop.run_in_executor(None, split_text_into_chunks, text, chunk_size) for text in texts]\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "async def get_embeddings_from_mistral(client, text_chunks):\n",
        "    response = client.embeddings(model=\"mistral-embed\", input=text_chunks)\n",
        "    return [embedding.embedding for embedding in response.data]\n",
        "\n",
        "async def fetch_and_process_data(search_query):\n",
        "    params = {\n",
        "        \"q\": search_query,  # query example\n",
        "        \"hl\": \"en\",         # language\n",
        "        \"gl\": \"uk\",         # country of the search, UK -> United Kingdom\n",
        "        \"start\": 0,         # number page by default up to 0\n",
        "        \"num\": 10           # parameter defines the maximum number of results to return per page.\n",
        "    }\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        page_num = 0\n",
        "        results = []\n",
        "\n",
        "        while len(results) < total_results_to_fetch:\n",
        "            page_num += 1\n",
        "            print('len results is : ',len(results), ' and total to fetch is ', total_results_to_fetch)\n",
        "            await fetch_page(session, params, page_num, results)\n",
        "\n",
        "        urls = [result['links'] for result in results]\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "            loop = asyncio.get_event_loop()\n",
        "            texts = await asyncio.gather(\n",
        "                *[loop.run_in_executor(executor, get_all_text_from_url, url) for url in urls]\n",
        "            )\n",
        "\n",
        "        chunks_list = await process_text_content(texts, chunk_size)\n",
        "\n",
        "        embeddings_list = []\n",
        "        for chunks in chunks_list:\n",
        "            embeddings = await get_embeddings_from_mistral(client, chunks)\n",
        "            embeddings_list.append(embeddings)\n",
        "\n",
        "        data = []\n",
        "        for i, result in enumerate(results):\n",
        "            if i >= len(embeddings_list):\n",
        "                print(f\"Error: No embeddings returned for result {i}\")\n",
        "                continue\n",
        "            for j, chunk in enumerate(chunks_list[i]):\n",
        "                if j >= len(embeddings_list[i]):\n",
        "                    print(f\"Error: No embedding returned for chunk {j} of result {i}\")\n",
        "                    continue\n",
        "                data.append({\n",
        "                    'title': result['title'],\n",
        "                    'url': result['links'],\n",
        "                    'chunk': chunk,\n",
        "                    'embedding': embeddings_list[i][j]\n",
        "                })\n",
        "\n",
        "        df = pd.DataFrame(data)\n",
        "        df.to_csv(dataframe_out_path, index=False)\n",
        "\n",
        "        # FAISS indexing\n",
        "        dimension = len(embeddings_list[0][0])  # assuming all embeddings have the same dimension\n",
        "        index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "        embeddings = np.array([entry['embedding'] for entry in data], dtype=np.float32)\n",
        "        index.add(embeddings)\n",
        "\n",
        "        faiss.write_index(index, faiss_index_path)\n",
        "\n",
        "await fetch_and_process_data(\"What is the latest news about AI and comfyui in the last 10 days?\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqmHZAI02TNw"
      },
      "source": [
        "## little embeddings and vector store creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "NTglR6v-2TNx",
        "outputId": "7d826534-f33f-467c-e0f6-661dd3e69979"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open faiss_index.index for reading: No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2394942edcdf>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AGI\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_vector_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-2394942edcdf>\u001b[0m in \u001b[0;36mquery_vector_store\u001b[0;34m(query_embedding, k)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Load the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaiss_index_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Ensure the query embedding is a numpy array with the correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/faiss/swigfaiss_avx2.py\u001b[0m in \u001b[0;36mread_index\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m  10536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10537\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10540\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char*) at /project/faiss/faiss/impl/io.cpp:67: Error: 'f' failed: could not open faiss_index.index for reading: No such file or directory"
          ]
        }
      ],
      "source": [
        "def query_vector_store(query_embedding, k=5):\n",
        "    \"\"\"\n",
        "    Query the FAISS vector store and return the text results along with metadata.\n",
        "\n",
        "    :param query_embedding: The embedding to query with.\n",
        "    :param k: Number of nearest neighbors to retrieve.\n",
        "    :return: List of dictionaries containing text results and metadata of the k nearest neighbors.\n",
        "    \"\"\"\n",
        "    # Load the index\n",
        "\n",
        "    index = faiss.read_index(faiss_index_path)\n",
        "\n",
        "    # Ensure the query embedding is a numpy array with the correct shape\n",
        "    if not isinstance(query_embedding, np.ndarray):\n",
        "        query_embedding = np.array(query_embedding, dtype=np.float32)\n",
        "    if query_embedding.ndim == 1:\n",
        "        query_embedding = np.expand_dims(query_embedding, axis=0)\n",
        "\n",
        "    # Query the index\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    # Load the dataframe\n",
        "    df = pd.read_csv(dataframe_out_path)\n",
        "\n",
        "    # Retrieve the text results and metadata\n",
        "    results = []\n",
        "    for idx in indices[0]:\n",
        "        result = {\n",
        "            'title': df.iloc[idx]['title'],\n",
        "            'url': df.iloc[idx]['url'],\n",
        "            'chunk': df.iloc[idx]['chunk']\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "def query_embeddings(texts):\n",
        "    \"\"\"\n",
        "    Convert text to embeddings using Mistral AI API.\n",
        "\n",
        "    :param api_key: Your Mistral API key.\n",
        "    :param texts: List of texts to convert to embeddings.\n",
        "    :return: List of embeddings.\n",
        "    \"\"\"\n",
        "    client = MistralClient(api_key=MISTRAL_API_KEY)\n",
        "    response = client.embeddings(model=\"mistral-embed\", input=[texts])\n",
        "    return [embedding.embedding for embedding in response.data]\n",
        "\n",
        "\n",
        "embeddings = query_embeddings(\"AGI\")\n",
        "results = query_vector_store(embeddings[0], k=5)\n",
        "results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMnL8INA2TNx"
      },
      "source": [
        "## tools definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXWd4P372TNy",
        "outputId": "cd7c2ce9-37f6-4802-9a7a-63b9c9183e31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching page: 1\n",
            "Fetching page: 2\n",
            "[{'title': \"Microsoft partners with France's Mistral AI, an OpenAI rival\", 'url': 'https://apnews.com/article/mistral-ai-lechat-microsoft-openai-cbd6f5604fa577a0d1e7f9047708b718', 'chunk': 'Mistral has also previously said it is teaming up with other big cloud providers including Amazon and Google.\\nRELATED COVERAGE\\nUS antitrust enforcers will investigate leading AI companies Microsoft, Nvidia and OpenAI\\nAI ‘gold rush’ for chatbot training data could run out of human-written text\\nFormer OpenAI employees lead push to protect whistleblowers flagging artificial intelligence risks\\nMistral made a big splash by attracting big amounts of investor funding to give it a multibillion-dollar valuation just months after it was founded last spring. It was started by three French former researchers from Google and Meta: CEO Arthur Mensch, Chief Scientist Guillaume Lample and Chief Technology Officer Timothee Lacroix.\\nIt has advertised an “open-source” approach to developing AI that involves publicly releasing key components of some AI systems, in contrast to companies such as OpenAI that closely guard them.'}, {'title': 'How Mistral AI, an OpenAI competitor, rocketed to $2Bn in ...', 'url': 'https://www.bensbites.com/case-study/how-mistral-ai-an-openai-competitor-rocketed-to-2bn-in-12-months', 'chunk': 'Something went wrong while submitting the form.PricingLog inSign upCase studies\\nHow Mistral AI, an OpenAI competitor, rocketed to $2Bn in <12 monthsHow Mistral AI, an OpenAI competitor, rocketed to $2Bn in <12 monthsSaveSavedToday Iâ\\x80\\x99m diving deep into Mistral AI, who are making headlines after recently closing their (huge) Series A round. Launched just 7 months ago, theyâ\\x80\\x99re disrupting the LLM market. I want to look at how theyâ\\x80\\x99re doing it - and how you can take advantage.This post covers:What is Mistral?Whoâ\\x80\\x99s behind it?The timeline: Whatâ\\x80\\x99s happened to dateFundraisingProduct OverviewA peek inside their seed deck ð\\x9f\\x91\\x80 Roadmap analysis. Are they achieving what they set out to do?5 big reasons Mistralâ\\x80\\x99s making waves ð\\x9f\\x8c\\x8aHow people actually use MistralOpportunities and how you can take advantageWhat developers think of MistralWhat is Mistral?A French startup that develops fast, open-source and secure language models.'}, {'title': 'How Mistral AI, an OpenAI competitor, rocketed to $2Bn in ...', 'url': 'https://www.bensbites.com/case-study/how-mistral-ai-an-openai-competitor-rocketed-to-2bn-in-12-months', 'chunk': 'As Mistral has been true to the promise of releasing open models, the community (and other companies) have taken Mistralâ\\x80\\x99s models and created better models on top of them. For example, OpenHermes 2.5 by Teknium and Neural Chat 7B by Intel.With AI models, open source takes many forms: from available to use locally but no details about the model (weights, architecture etc.) to models that are fully open source and allow users to train on the outputs.While Mistralâ\\x80\\x99s models were open-weights from the start, Mistralâ\\x80\\x99s latest announcement had a line in their Terms of Service Terms of Service which was spotted by Far El on Twitter, said:Basically, you canâ\\x80\\x99t use it to train or improve other models or compete against themâ\\x80¦It wasnâ\\x80\\x99t clear whether this was just for the API platform or the model itself. And unfortunately, open-source means you should be able to use this tech how you like, thatâ\\x80\\x99s kind of the point.'}, {'title': 'How Mistral AI, an OpenAI competitor, rocketed to $2Bn in ...', 'url': 'https://www.bensbites.com/case-study/how-mistral-ai-an-openai-competitor-rocketed-to-2bn-in-12-months', 'chunk': 'Founded in 2023 by Arthur Mensch, Guillaume Lample, and TimothÃ©e Lacroix.Theyâ\\x80\\x99ve raised over $650M in funding, are valued at $2Bn, are less than a year old and have 22 employees.monthly search volume for â\\x80\\x98mistral aiâ\\x80\\x99The company is important for a few reasons;Itâ\\x80\\x99s actually open-source, you know like OpenAI was supposed to be? Or how LlaMA by Meta kinda is but isnâ\\x80\\x99t?Itâ\\x80\\x99s developed 2 AI models in less than a year.Itâ\\x80\\x99s French.The founders are 3 researchers from DeepMind and Meta who aimed to beat GPT 3.5 by year-end.'}, {'title': 'How Mistral AI, an OpenAI competitor, rocketed to $2Bn in ...', 'url': 'https://www.bensbites.com/case-study/how-mistral-ai-an-openai-competitor-rocketed-to-2bn-in-12-months', 'chunk': 'The big debate.Mistral believes (as do many others, myself included) that there are several concerns with closed AI approaches; businesses have to send sensitive data to it, only exposing the outputs doesnâ\\x80\\x99t help connect with other components (retrieval, structure inputs etc) and the data used to train the models are secret (so we assume it can do some things it perhaps hasnâ\\x80\\x99t been trained on).Now the bold stuff.â\\x80\\x9cMistral will offer the best technology in 4 yearsâ\\x80\\x9d.How?Theyâ\\x80\\x99ll take a more open approach to model development.Tighter integration with customersâ\\x80\\x99 workflows.Increase focus on data sources and control.Propose unmatched guarantees on security and privacy.Thereâ\\x80\\x99s a lot more detail in their deck on the above 4 points.As far as business focus goesâ\\x80¦â\\x80\\x9cOn the business side, we will provide the most valuable technology brick to the emerging AI-as-a-service industry that will revolutionise business workflows with generative AI.'}]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "tools = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"mistral_web_search\",\n",
        "            \"description\": \"Fetch and process data from Google search based on a query, store results in FAISS vector store, and retrieve results.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"search_query\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The search query to use for fetching data from Google search.\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"search_query\"]\n",
        "            },\n",
        "        },\n",
        "    },\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mistral_web_search(search_query: str):\n",
        "    async def run_search():\n",
        "        await fetch_and_process_data(search_query)\n",
        "        embeddings = query_embeddings(search_query)\n",
        "        results_ = query_vector_store(embeddings[0], k=5)\n",
        "        return results_\n",
        "\n",
        "    return asyncio.run(run_search())\n",
        "\n",
        "search_query = \"mistral and openai\"\n",
        "results = mistral_web_search(search_query)\n",
        "print(results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxHO6I3k2TNz",
        "outputId": "76fd5df7-a2b1-471c-a494-7df577f396f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching page: 1\n",
            "Fetching page: 2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'Mistral has also previously said it is teaming up with other big cloud providers including Amazon and Google.\\nRELATED COVERAGE\\nUS antitrust enforcers will investigate leading AI companies Microsoft, Nvidia and OpenAI\\nAI ‘gold rush’ for chatbot training data could run out of human-written text\\nFormer OpenAI employees lead push to protect whistleblowers flagging artificial intelligence risks\\nMistral made a big splash by attracting big amounts of investor funding to give it a multibillion-dollar valuation just months after it was founded last spring. It was started by three French former researchers from Google and Meta: CEO Arthur Mensch, Chief Scientist Guillaume Lample and Chief Technology Officer Timothee Lacroix.\\nIt has advertised an “open-source” approach to developing AI that involves publicly releasing key components of some AI systems, in contrast to companies such as OpenAI that closely guard them.\\nSomething went wrong while submitting the form.PricingLog inSign upCase studies\\nHow Mistral AI, an OpenAI competitor, rocketed to $2Bn in <12 monthsHow Mistral AI, an OpenAI competitor, rocketed to $2Bn in <12 monthsSaveSavedToday Iâ\\x80\\x99m diving deep into Mistral AI, who are making headlines after recently closing their (huge) Series A round. Launched just 7 months ago, theyâ\\x80\\x99re disrupting the LLM market. I want to look at how theyâ\\x80\\x99re doing it - and how you can take advantage.This post covers:What is Mistral?Whoâ\\x80\\x99s behind it?The timeline: Whatâ\\x80\\x99s happened to dateFundraisingProduct OverviewA peek inside their seed deck ð\\x9f\\x91\\x80 Roadmap analysis. Are they achieving what they set out to do?5 big reasons Mistralâ\\x80\\x99s making waves ð\\x9f\\x8c\\x8aHow people actually use MistralOpportunities and how you can take advantageWhat developers think of MistralWhat is Mistral?A French startup that develops fast, open-source and secure language models.\\nAs Mistral has been true to the promise of releasing open models, the community (and other companies) have taken Mistralâ\\x80\\x99s models and created better models on top of them. For example, OpenHermes 2.5 by Teknium and Neural Chat 7B by Intel.With AI models, open source takes many forms: from available to use locally but no details about the model (weights, architecture etc.) to models that are fully open source and allow users to train on the outputs.While Mistralâ\\x80\\x99s models were open-weights from the start, Mistralâ\\x80\\x99s latest announcement had a line in their Terms of Service Terms of Service which was spotted by Far El on Twitter, said:Basically, you canâ\\x80\\x99t use it to train or improve other models or compete against themâ\\x80¦It wasnâ\\x80\\x99t clear whether this was just for the API platform or the model itself. And unfortunately, open-source means you should be able to use this tech how you like, thatâ\\x80\\x99s kind of the point.\\nFounded in 2023 by Arthur Mensch, Guillaume Lample, and TimothÃ©e Lacroix.Theyâ\\x80\\x99ve raised over $650M in funding, are valued at $2Bn, are less than a year old and have 22 employees.monthly search volume for â\\x80\\x98mistral aiâ\\x80\\x99The company is important for a few reasons;Itâ\\x80\\x99s actually open-source, you know like OpenAI was supposed to be? Or how LlaMA by Meta kinda is but isnâ\\x80\\x99t?Itâ\\x80\\x99s developed 2 AI models in less than a year.Itâ\\x80\\x99s French.The founders are 3 researchers from DeepMind and Meta who aimed to beat GPT 3.5 by year-end.\\nThe big debate.Mistral believes (as do many others, myself included) that there are several concerns with closed AI approaches; businesses have to send sensitive data to it, only exposing the outputs doesnâ\\x80\\x99t help connect with other components (retrieval, structure inputs etc) and the data used to train the models are secret (so we assume it can do some things it perhaps hasnâ\\x80\\x99t been trained on).Now the bold stuff.â\\x80\\x9cMistral will offer the best technology in 4 yearsâ\\x80\\x9d.How?Theyâ\\x80\\x99ll take a more open approach to model development.Tighter integration with customersâ\\x80\\x99 workflows.Increase focus on data sources and control.Propose unmatched guarantees on security and privacy.Thereâ\\x80\\x99s a lot more detail in their deck on the above 4 points.As far as business focus goesâ\\x80¦â\\x80\\x9cOn the business side, we will provide the most valuable technology brick to the emerging AI-as-a-service industry that will revolutionise business workflows with generative AI.'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\" little helper function to extract only the texts \"\"\"\n",
        "def tools_to_str(tools_output: list) -> str:\n",
        "    return '\\n'.join([tool['chunk'] for tool in tools_output])\n",
        "\n",
        "\n",
        "tools_to_str(mistral_web_search(search_query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGZ7dv8a2TN1"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "names_to_functions = {\n",
        "    'mistral_web_search': functools.partial(mistral_web_search),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pB7_DDCT2TN2"
      },
      "source": [
        "## chat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfF_9JUx2TN3"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    ChatMessage(role=\"user\", content=\"What happend during apple WWDC 2024?\"),\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk2HwQnJ2TN4",
        "outputId": "6e4e0e06-4b92-4432-e911-e2f4f7426603"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletionResponse(id='ed2e245edaf04b3e96fea1a914e9e97f', object='chat.completion', created=1718097895, model='mistral-large-latest', choices=[ChatCompletionResponseChoice(index=0, message=ChatMessage(role='assistant', content='', name=None, tool_calls=[ToolCall(id='wq5uJdILV', type=<ToolType.function: 'function'>, function=FunctionCall(name='mistral_web_search', arguments='{\"search_query\": \"apple WWDC 2024\"}'))], tool_call_id=None), finish_reason=<FinishReason.tool_calls: 'tool_calls'>)], usage=UsageInfo(prompt_tokens=121, total_tokens=156, completion_tokens=35))"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = \"mistral-large-latest\"\n",
        "\n",
        "client = MistralClient(api_key=MISTRAL_API_KEY)\n",
        "response = client.chat(model=model, messages=messages, tools=tools, tool_choice=\"any\")\n",
        "response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QZojZ872TN5"
      },
      "outputs": [],
      "source": [
        "messages.append(response.choices[0].message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VUjl6aB2TN5",
        "outputId": "2d102b3f-ef16-4d4a-b59a-d39d8081442a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "function_name:  mistral_web_search \n",
            "function_params:  {'search_query': 'apple WWDC 2024'}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "tool_call = response.choices[0].message.tool_calls[0]\n",
        "function_name = tool_call.function.name\n",
        "function_params = json.loads(tool_call.function.arguments)\n",
        "\n",
        "\n",
        "print(\"\\nfunction_name: \", function_name, \"\\nfunction_params: \", function_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nyhd_kdn2TN6",
        "outputId": "7a7a2aba-9b14-445e-89e7-1d7c300a4b6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching page: 1\n",
            "Fetching page: 2\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "\"Apple WWDC 2024: the 13 biggest announcements - The VergeSkip to main contentThe VergeThe Verge logo.The Verge homepageThe Verge homepageThe VergeThe Verge logo./Tech/Reviews/Science/Entertainment/AI/MoreMenuExpandThe VergeThe Verge logo.MenuExpandWWDC 2024/Apple/TechApple WWDC 2024: the 13 biggest announcementsApple WWDC 2024: the 13 biggest announcements / Apple’s WWDC keynote had a lot to do with AI.By\\nEmma Roth, a news writer who covers the streaming wars, consumer tech, crypto, social media, and much more. Previously, she was a writer and editor at MUO.\\nJun 10, 2024, 6:57 PM UTCShare this storyApple’s Worldwide Developers Conference keynote has come to a close — and the company had a whole lot to share.\\nFifty Distinguished Winners, who are recognised for outstanding submissions, will be invited to Cupertino for a three-day experience.\\nApple will share additional conference information in advance of WWDC24 through the Apple Developer app and website.\\nShare article\\nMedia\\nText of this article\\n26 March 2024\\nPRESS RELEASE\\nApple’s Worldwide Developers Conference returns June 10, 2024\\nEntire conference available online for all developers, with a special event at Apple Park on June 10\\nCUPERTINO, CALIFORNIA Apple today announced it will host its annual Worldwide Developers Conference (WWDC) online from June 10 through 14, 2024. Developers and students will have the opportunity to celebrate in person at a special event at Apple Park on opening day.\\nFree for all developers, WWDC24 will spotlight the latest iOS, iPadOS, macOS, watchOS, tvOS, and visionOS advancements.\\niPadOS could finally get new apps like a native calculator, and some rumored new Apple Pencil 3 features.\\xa0We expect macOS to offer AI features, with native apps that lean on smart tools and generative AI: assisted writing in Pages, a slide deck maker in Keynote, coding in Xcode, all powered by AI.\\xa0When it comes to the Vision Pro and visionOS, we haven't heard much, so we're expecting a nod to spatial computing, but nothing huge coming yet. For wearables, we're not expecting big watchOS changes with watchOS 11, either.Watch WWDC 2024 live with usWWDC 2024 as it happens\\nRefresh\\n2024-06-10T16:26:28.008Z\\nGood morning, good afternoon and good evening, wherever you are in the world – and welcome to TechRadar's live coverage of WWDC 2024.\\n2024-06-10T16:45:58.710Z\\n(Image credit: Jacob Krol / Future)Apple's WWDC 2024 keynote is minutes away, with rumors swirling about \\xa0announcements focusing on artificial intelligence and Siri, Apple's languishing voice assistant.\\nThis year’s conference will include video sessions and opportunities to engage with Apple designers and engineers and connect with the worldwide developer community.\\nWWDC24 will include an in-person experience on June 10 that will provide developers the opportunity to watch the keynote at Apple Park, meet with Apple team members, and take part in special activities. Space will be limited, and details on how to apply to attend can be found on the Apple Developer site and app.\\nApple is proud to support the next generation of developers through the Swift Student Challenge, one of many Apple programs that seek to uplift the next generation of developers, creators, and entrepreneurs. On March 28, this year’s applicants will be notified of their status, and winners will be eligible to apply for the in-person experience at Apple Park. Fifty Distinguished Winners, who are recognised for outstanding submissions, will be invited to Cupertino for a three-day experience.\\nThis year’s conference will include video sessions and opportunities to engage with Apple designers and engineers and connect with the worldwide developer community.\\nWWDC24 will include an in-person experience on June 10 that will provide developers the opportunity to watch the keynote at Apple Park, meet with Apple team members, and take part in special activities. Space will be limited, and details on how to apply to attend can be found on the Apple Developer site and app.\\nApple is proud to support the next generation of developers through the Swift Student Challenge, one of many Apple programs that seek to uplift the next generation of developers, creators, and entrepreneurs. On March 28, this year’s applicants will be notified of their status, and winners will be eligible to apply for the in-person experience at Apple Park.\""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "function_result = tools_to_str(names_to_functions[function_name](**function_params))\n",
        "function_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKPdch2A2TN6",
        "outputId": "03f2048f-d0ae-4370-93f4-dc59f4494161"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Apple's Worldwide Developers Conference (WWDC) took place on June 10, 2024. The keynote focused mainly on artificial intelligence, with several announcements made. Unfortunately, I don't have real-time information, so I can't provide the specific details of the 13 biggest announcements mentioned in the article from The Verge.\\n\\nHowever, some general expectations before the event included the possibility of new apps for iPadOS such as a native calculator and new features for Apple Pencil 3. For macOS, it was anticipated that there would be AI features and smart tools integrated into native apps like Pages, Keynote, and Xcode.\\n\\nThe event also included video sessions and opportunities for developers to engage with Apple designers and engineers. There was an in-person experience at Apple Park for selected developers to watch the keynote, meet with Apple team members, and participate in special activities. Apple also continued its tradition of supporting the next generation of developers through the Swift Student Challenge.\""
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages.append(ChatMessage(role=\"tool\", name=function_name, content=function_result, tool_call_id=tool_call.id))\n",
        "\n",
        "response = client.chat(model=model, messages=messages)\n",
        "response.choices[0].message.content\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGaLB3wG2TN7"
      },
      "source": [
        "## Chat in a chain (cleaner user experience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq8EFnWe2TN7"
      },
      "outputs": [],
      "source": [
        "messages = []\n",
        "\n",
        "while True:\n",
        "    input_ = input(\"Ask: \")\n",
        "    messages.append(ChatMessage(role=\"user\", content=input_))\n",
        "    response = client.chat(model=model, messages=messages, tools=tools, tool_choice=\"any\")\n",
        "    messages.append(response.choices[0].message)\n",
        "    print(response.choices[0].message.content)\n",
        "    tool_call = response.choices[0].message.tool_calls[0]\n",
        "    function_name = tool_call.function.name\n",
        "    function_params = json.loads(tool_call.function.arguments)\n",
        "\n",
        "    function_result_raw = names_to_functions[function_name](**function_params)\n",
        "    print(\"sources: \", [f\"{source['title']} - {source['url']}\" for source in function_result_raw])\n",
        "    function_result_text = tools_to_str(function_result_raw)\n",
        "    messages.append(ChatMessage(role=\"tool\", name=function_name, content=function_result_text, tool_call_id=tool_call.id))\n",
        "\n",
        "    response = client.chat(model=model, messages=messages)\n",
        "    final_response = response.choices[0].message.content\n",
        "    print(final_response)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mistral-cookbook-contrib",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}